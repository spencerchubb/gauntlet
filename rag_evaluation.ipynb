{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open(\".env\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"export \", \"\")\n",
    "        key, value = line.split(\"=\")\n",
    "        os.environ[key] = value.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_QAs = [\n",
    "    # GauntletAI Project 1 - ChatGenius.txt\n",
    "    (\"What features do we need in the Slack clone?\", \"\"\"Authentiication, Real-time messaging, Channel/DM organization, File sharing & search, User presence & status, Thread support, Emoji reactions\"\"\"),\n",
    "    (\"When is the first MVP due?\", \"There is a checkpoint deadline on January 7th, and the full MVP is due January 10th\"),\n",
    "    (\"What are some AI features we could add after the MVP?\", \"AI that talks for the user, incorporate context into the AI, make a voice and video avatar for the user, mirror the user's personality (not an exhaustive list)\"),\n",
    "\n",
    "    # gauntletai.com.txt\n",
    "    (\"What is Gauntlet AI?\", \"Gauntlet AI is an extremely intensive 12-week AI training to turn engineers into the most sought-after builders and entrepreneurs on the planet\"),\n",
    "    (\"How much will Gauntlet graduates get paid?\", \"$200k/yr\"),\n",
    "    (\"Where will Gauntlet grads work when it's done?\", \"Austin, TX\"),\n",
    "    (\"Can Gauntlet grads work remotely when it's done?\", \"In the vast majority of cases, jobs are in Austin, TX, not remote. There are a few exceptions for European students\"),\n",
    "\n",
    "    # GMT20250106-150124_Recording.cc.vtt\n",
    "    (\"Who are the hiring companies?\", \"Zax software, Alpha School, and the third has yet to be announced\"),\n",
    "    (\"How do we override the concensus view of an LLM?\", \"We make a brainlift with some Spiky POVs and feed that to the model\"),\n",
    "    (\"How can we protect our privacy while using WorkSmart?\", \"You can turn it off while doing personal stuff, use a separate user on your computer, and/or delete logs that you don't want stored\"),\n",
    "\n",
    "    # GMT20250106-200212_Recording.cc.vtt\n",
    "    (\"What is Alpha School?\", \"Virtual charter school that recently launched. Location such as Arizona, Austin, Miami, and Brownsville. Aims to fix education. Students have 2 hours of academic per day, and still outperform other schools\"),\n",
    "    (\"How is Alpha School different from competitors?\", \"It's designed for the kids who don't typically love school, and aren't already conscientious learners\"),\n",
    "\n",
    "    # GMT20250107-160241_Recording.cc.vtt\n",
    "    (\"Is there a limit to what we can use on AWS?\", \"Gauntlet wants to give you control, but please use discretion. For example, don't spin up a ton of EC2 instances\"),\n",
    "    (\"What should we do if AI is unable to solve a problem?\", \"Go to a checkpoint that was working, have AI look at the specific lines causing the issue, and if you keep trying for a while, don't be afraid to ask for help\"),\n",
    "\n",
    "    # GMT20250108-160105_Recording.cc.vtt\n",
    "    (\"What is a Spiky POV?\", \"A belief that is non-consensus. LLMs would not typically output a Spiky POV because it wasn't in their training data\"),\n",
    "    (\"How do we make a BrainLift?\", \"Follow a specific format in a Workflowy document\"),\n",
    "\n",
    "    # GMT20250108-225810_Recording.cc.vtt\n",
    "    (\"Can we use other cloud services besides AWS?\", \"Yes, you can use any cloud services. If you want a free option, Gauntlet pays for AWS for you\"),\n",
    "\n",
    "    # GMT20250109-190113_Recording.cc.vtt\n",
    "    (\"What is ZAX Software?\", \"Zax software is going to buy businesses and run them extremely efficiently using AI. With automation, 1 person can run a whole company\"),\n",
    "    (\"Why should someone work for ZAX?\", \"The compensation will include a profit share, enabling you to possibly make a million dollars a year\"),\n",
    "    (\"Why should I build a company in ZAX instead of on my own?\", \"It's less risky because we pay cash compensation in addition to the profit share\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_QAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/cookbook/en/rag_evaluation\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "chunk_size = 200\n",
    "tokenizer_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=int(chunk_size / 10),\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vtt(filename):\n",
    "    with open(f\"rag_data/{filename}\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    lines = [line for line in lines if line]\n",
    "    lines = [line for line in lines if not line.isdigit()] # Remove if it's an integer\n",
    "    lines = lines[1:] # First line is just \"WEBVTT\"\n",
    "    lines = lines[1::2] # Every other line is a timestamp\n",
    "    text = \"\\n\".join(lines)\n",
    "    return text\n",
    "# print(parse_vtt(\"GMT20250106-150124_Recording.cc.vtt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "texts = []\n",
    "for file in os.listdir(\"rag_data\"):\n",
    "    if file.endswith(\".vtt\"):\n",
    "        texts.append(parse_vtt(file))\n",
    "    else:\n",
    "        texts.append(open(f\"rag_data/{file}\", \"r\").read())\n",
    "\n",
    "texts = [text.replace(\"Zach's\", \"Zax\") for text in texts]\n",
    "texts = [text.replace(\"Zach Software\", \"Zax Software\") for text in texts]\n",
    "\n",
    "chunks = text_splitter.create_documents(texts)\n",
    "chunks = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.rag import add_documents, similarity_search\n",
    "\n",
    "add_documents(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.completion import bedrock_completion\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"question\", \"reference_answer\", \"llm_answer\"])\n",
    "\n",
    "for question, reference_answer in eval_QAs:\n",
    "    print(question)\n",
    "\n",
    "    # Without HyDE\n",
    "    # hypothetical_document = \"\"\n",
    "\n",
    "    # With HyDE\n",
    "    hypothetical_document = bedrock_completion(\n",
    "        \"\"\"You are a question answering assistant for Gauntlet AI, an intensive AI training program for engineers.\n",
    "Answer length MUST be 1 sentence to 1 paragraph in length. Answer questions with a decisive and convincing answer.\n",
    "Do NOT express uncertainty, NEVER say you don't know something.\n",
    "\"\"\",\n",
    "        [{\"role\": \"user\", \"content\": question}],\n",
    "        \"llama3-2-3b\",\n",
    "    )\n",
    "    hypothetical_document = f\"\\n{hypothetical_document}\"\n",
    "    print(hypothetical_document)\n",
    "\n",
    "    context = \"\"\n",
    "    docs = similarity_search(question + hypothetical_document)\n",
    "    for doc in docs:\n",
    "        context += f\"{doc}\\n\"\n",
    "    prompt = f\"\"\"### Instructions\n",
    "You are a question-answering assistant. You will be given a question and context.\n",
    "For questions involving dates or times, give absolute answers instead of relative answers if possible (e.g. \"3pm\" instead of \"in 2 hours\").\n",
    "Answer the question ONLY using the context. If the context does not contain the answer, say \"I don't know\".\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "### Answer\n",
    "\"\"\"\n",
    "    llm_answer = bedrock_completion(\n",
    "        \"You are a question-answering assistant.\",\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"llama3-3-70b\",\n",
    "    )\n",
    "    \n",
    "    # Add row to dataframe\n",
    "    df = pd.concat([df, pd.DataFrame({\n",
    "        \"question\": [question],\n",
    "        \"reference_answer\": [reference_answer], \n",
    "        \"llm_answer\": [llm_answer]\n",
    "    })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Judge the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"feedback\"] = None\n",
    "df[\"score\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(df.values):\n",
    "    question = row[0]\n",
    "    reference_answer = row[1]\n",
    "    llm_answer = row[2]\n",
    "    feedback = row[3] if len(row) > 3 else None\n",
    "    score = row[4] if len(row) > 4 else None\n",
    "    if feedback and score and pd.notna(feedback) and pd.notna(score):\n",
    "        continue\n",
    "\n",
    "    prompt = f\"\"\"###Task Description:\n",
    "You will be given a question, a response to evaluate, a reference answer that gets a score of 5, and a score rubric.\n",
    "1. Write detailed feedback that assess the quality of the response ONLY based on the given score rubric, not evaluating in general.\n",
    "2. After writing feedback, write a score that is an integer between 1 and 5.\n",
    "3. The output format must be: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Follow the output format exactly as I asked for, no more and no less. Be sure to include [RESULT] in your output.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Response to evaluate\n",
    "{llm_answer}\n",
    "\n",
    "### Reference Answer (This would earn a score of 5)\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: Completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: Mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: Somewhat correct, accurate, and/or factual.\n",
    "Score 4: Mostly correct, accurate, and factual.\n",
    "Score 5: Completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "    judge_response = bedrock_completion(\n",
    "        \"You are a fair judge.\",\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"llama3-3-70b\",\n",
    "    )\n",
    "    try:\n",
    "        feedback = judge_response.split(\"Feedback: \")[1].split(\"[RESULT]\")[0]\n",
    "        score = int(judge_response.split(\"[RESULT]\")[1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(judge_response)\n",
    "        print()\n",
    "    print(score)\n",
    "    df.loc[i, \"feedback\"] = feedback\n",
    "    df.loc[i, \"score\"] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.score.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[df.score < 3].values:\n",
    "    print(row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.completion import bedrock_completion\n",
    "import random\n",
    "\n",
    "# for i in range(40):\n",
    "#     print(i)\n",
    "random_chunk = random.choice(chunks)\n",
    "\n",
    "qa = bedrock_completion(QA_generation_prompt, [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Context: {random_chunk}\\n\\nOutput:::\",\n",
    "    },\n",
    "], model_id=\"llama3-3-70b\")\n",
    "factoid_question, answer = qa.split(\"\\n\")\n",
    "factoid_question = factoid_question.replace(\"Factoid question: \", \"\")\n",
    "answer = answer.replace(\"Answer: \", \"\")\n",
    "print(factoid_question)\n",
    "print()\n",
    "print(answer)\n",
    "print()\n",
    "print(random_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"Whatever you want on it, for example MongoDB or Postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame({\"question\": [factoid_question], \"answer\": [answer], \"context\": [random_chunk]})], ignore_index=True)\n",
    "print(len(df))\n",
    "df.to_csv(\"rag_evaluation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
